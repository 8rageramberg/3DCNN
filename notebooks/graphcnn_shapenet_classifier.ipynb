{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1340ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: /Users/brageramberg/Desktop/3DCNN/data/shapeNetCore\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from pytorch3d.datasets import ShapeNetCore\n",
    "from pytorch3d.datasets.utils import collate_batched_meshes\n",
    "from pytorch3d.ops import GraphConv\n",
    "from collections import defaultdict\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from pytorch3d.datasets.utils import collate_batched_meshes\n",
    "\n",
    "from pytorch3d.structures import Meshes\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "# Root path to your local ShapeNetCore.v2 directory.\n",
    "# Example folder structure: SHAPENET_PATH/02958343/<model_id>/* etc.\n",
    "SHAPENET_PATH = Path(\"../data/shapeNetCore\").expanduser().resolve()\n",
    "assert SHAPENET_PATH.exists(), f\"ShapeNetCore folder not found: {SHAPENET_PATH}\"\n",
    "print(\"Dataset root:\", SHAPENET_PATH)\n",
    "\n",
    "# Choose 5 synsets by default (airplane, chair, lamp, mug, table)\n",
    "# Feel free to edit this to the categories you actually want to use.\n",
    "CATEGORIES = {\n",
    "    \"02808440\": \"bathtub\",\n",
    "    \"02992529\": \"cellphone\",\n",
    "    \"03046257\": \"clock\",\n",
    "    \"03211117\": \"display\",\n",
    "    \"03642806\": \"laptop\",\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(CATEGORIES)\n",
    "\n",
    "# Data / training params\n",
    "BATCH_SIZE = 4\n",
    "VAL_SPLIT = 0.2\n",
    "NUM_WORKERS = 2\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "SEED = 42\n",
    "\n",
    "device = (torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "          else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "          else torch.device(\"cpu\"))\n",
    "print(\"Device:\", device)\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22cd62",
   "metadata": {},
   "source": [
    "## Dataset & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1854f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(x: torch.Tensor, num_verts_per_mesh: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: (sum(V_i), F) vertex features packed across the batch\n",
    "    num_verts_per_mesh: (B,) number of vertices per mesh\n",
    "    returns: (B, F) mean pooled features per mesh\n",
    "    \"\"\"\n",
    "    # Build batch index: for mesh i, repeat index i num_verts_per_mesh[i] times\n",
    "    batch_index = torch.cat([\n",
    "        torch.full((int(n),), i, device=x.device, dtype=torch.long)\n",
    "        for i, n in enumerate(num_verts_per_mesh)\n",
    "    ], dim=0)  # (sum(V_i),)\n",
    "\n",
    "    B = int(num_verts_per_mesh.numel())\n",
    "    Fdim = x.size(1)\n",
    "    out = x.new_zeros((B, Fdim))\n",
    "    out.index_add_(0, batch_index, x)  # sum over vertices for each mesh\n",
    "    out = out / num_verts_per_mesh.view(-1, 1).to(x.dtype)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6db597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples found: 3891\n",
      "Train: 3113, Val: 778\n",
      "train_loader workers: 0\n",
      "val_loader workers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brageramberg/opt/miniconda3/envs/pytorch3d/lib/python3.10/site-packages/pytorch3d/datasets/shapenet/shapenet_core.py:116: UserWarning: The following categories are included in ShapeNetCore ver.2's official mapping but not found in the dataset location /Users/brageramberg/Desktop/3DCNN/data/shapeNetCore: 03207941, 02946921, 04090263, 04225987, 03691459, 02954340, 03001627, 03624134, 03759954, 03928116, 02828884, 03761084, 04554684, 04460130, 02942699, 04530566, 03513137, 04330267, 04256520, 03991062, 04379243, 02691156, 04401088, 02880940, 02801938, 03938244, 02747177, 02924116, 03636649, 03948459, 02818832, 03085013, 03337140, 02933112, 02871439, 02876657, 04074963, 04099429, 04004475, 02843684, 03797390, 02958343, 03261776, 03467517, 03325088, 03593526, 03790512, 03710193, 04468005, 02773838\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load ShapeNetCore with selected synsets.\n",
    "# Setting load_textures=False avoids texture loading overhead/issues.\n",
    "dataset = ShapeNetCore(\n",
    "    data_dir=str(SHAPENET_PATH),                # <-- was root=\n",
    "    synsets=list(CATEGORIES.keys()),            # <-- pass a list of synset ids\n",
    "    version=2,\n",
    "    load_textures=False\n",
    ")\n",
    "\n",
    "print(f\"Total samples found: {len(dataset)}\")\n",
    "\n",
    "# Split into train/val\n",
    "val_size = int(len(dataset) * VAL_SPLIT)\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# Build synset->index mapping for labels\n",
    "synset_list = list(CATEGORIES.keys())  # preserves your chosen order\n",
    "synset_to_idx = {sid: i for i, sid in enumerate(synset_list)}\n",
    "idx_to_synset = {i: sid for sid, i in synset_to_idx.items()}\n",
    "idx_to_name = {i: CATEGORIES[sid] for i, sid in idx_to_synset.items()}\n",
    "\n",
    "\n",
    "def normalize_verts(v):\n",
    "    c = v.mean(0, keepdim=True)\n",
    "    v = v - c\n",
    "    s = v.norm(dim=1).max().clamp(min=1e-6)\n",
    "    return v / s\n",
    "\n",
    "\n",
    "\n",
    "def decimate_mesh(verts, faces, max_faces=1500):\n",
    "    F = faces.shape[0]\n",
    "    if F <= max_faces:\n",
    "        return verts, faces\n",
    "    idx = torch.randperm(F)[:max_faces]\n",
    "    f_small = faces[idx]\n",
    "    used, inv = torch.unique(f_small.reshape(-1), sorted=True, return_inverse=True)\n",
    "    v_small = verts[used]\n",
    "    f_small = inv.reshape(-1, 3)\n",
    "    return v_small, f_small\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    V, F = [], []\n",
    "    for s in batch:\n",
    "        v, f = decimate_mesh(s[\"verts\"], s[\"faces\"], max_faces=1500)\n",
    "        V.append(normalize_verts(v))\n",
    "        F.append(f)\n",
    "    meshes = Meshes(verts=V, faces=F)\n",
    "    labels = torch.tensor([synset_to_idx[s[\"synset_id\"]] for s in batch], dtype=torch.long)\n",
    "    return {\"meshes\": meshes, \"labels\": labels}\n",
    "\n",
    "# Nuke old references so we don't accidentally use them\n",
    "try:\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# (Re)build loaders with num_workers=0 in notebooks/macOS\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=0, collate_fn=custom_collate_fn,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, collate_fn=custom_collate_fn,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "print(\"train_loader workers:\", train_loader.num_workers)\n",
    "print(\"val_loader workers:\",   val_loader.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8ed26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pytorch3d.structures.meshes.Meshes'>\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(val_loader))\n",
    "print(type(b[\"meshes\"]))         # should be <class 'pytorch3d.structures.meshes.Meshes'>\n",
    "print(b[\"labels\"].shape)         # torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b31617",
   "metadata": {},
   "source": [
    "## Model: Simple GraphCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc7827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGraphCNN(\n",
      "  (conv1): GraphConv(3 -> 64, directed=False)\n",
      "  (conv2): GraphConv(64 -> 128, directed=False)\n",
      "  (conv3): GraphConv(128 -> 256, directed=False)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SimpleGraphCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphConv classifier:\n",
    "    - per-vertex input: (x, y, z)\n",
    "    - 3 GraphConv layers\n",
    "    - global mean pooling per mesh (pure PyTorch)\n",
    "    - MLP head -> NUM_CLASSES\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(3, 64)\n",
    "        self.conv2 = GraphConv(64, 128)\n",
    "        self.conv3 = GraphConv(128, 256)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, meshes):\n",
    "        x = meshes.verts_packed()     # (sum(V_i), 3)\n",
    "        edges = meshes.edges_packed() # (sum(E_i), 2) int64\n",
    "\n",
    "        x = F.relu(self.conv1(x, edges))\n",
    "        x = F.relu(self.conv2(x, edges))\n",
    "        x = F.relu(self.conv3(x, edges))\n",
    "\n",
    "        # Global mean pool per mesh w/o torch_scatter\n",
    "        num_verts_per_mesh = meshes.num_verts_per_mesh()  # (B,)\n",
    "        avg_features = mean_pool(x, num_verts_per_mesh)   # (B, 256)\n",
    "\n",
    "        h = F.relu(self.fc1(avg_features))\n",
    "        logits = self.fc2(h)\n",
    "        return logits\n",
    "\n",
    "model = SimpleGraphCNN(NUM_CLASSES).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97e65b",
   "metadata": {},
   "source": [
    "## Training & Evaluation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82afd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training setup (replace this whole block) ---\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# Keep your existing 'device' from earlier; don't redefine it here\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "use_cuda = (device.type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler(device_type=\"cuda\") if use_cuda else None  # CUDA only\n",
    "\n",
    "PRINT_EVERY = 25  # batches\n",
    "\n",
    "def run_epoch(loader, train: bool = True):\n",
    "    model.train(train)\n",
    "    running_loss, running_correct, n_samples = 0.0, 0, 0\n",
    "\n",
    "    autocast = (torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
    "                if device.type == \"cuda\" else nullcontext())\n",
    "\n",
    "    start = time.time()\n",
    "    for bi, batch in enumerate(loader, 1):\n",
    "        t0 = time.time()\n",
    "        meshes = batch[\"meshes\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        bs = labels.size(0)\n",
    "        n_samples += bs\n",
    "\n",
    "        with autocast:\n",
    "            logits = model(meshes)\n",
    "            logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if device.type == \"cuda\" and scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * bs\n",
    "        running_correct += (logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        if bi % PRINT_EVERY == 0 or bi == len(loader):\n",
    "            dt = time.time() - start\n",
    "            avg_bt = dt / bi\n",
    "            eta = avg_bt * (len(loader) - bi)\n",
    "            print(f\"[{'train' if train else 'val':5}] \"\n",
    "                  f\"batch {bi:4d}/{len(loader)} | \"\n",
    "                  f\"avg {avg_bt:.2f}s/batch | ETA {eta/60:.1f}m\")\n",
    "\n",
    "    epoch_loss = running_loss / max(1, n_samples)\n",
    "    epoch_acc  = running_correct / max(1, n_samples)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def train_loop(epochs=EPOCHS):\n",
    "    best_val_acc = 0.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "        val_loss,   val_acc   = run_epoch(val_loader,   train=False)\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        print(f\"Epoch {ep:02d}/{epochs} | \"\n",
    "              f\"train_loss={train_loss:.4f} acc={train_acc:.3f} | \"\n",
    "              f\"val_loss={val_loss:.4f} acc={val_acc:.3f} | \"\n",
    "              f\"{dt:.1f}s\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"synset_to_idx\": synset_to_idx,\n",
    "                \"config\": {\"CATEGORIES\": CATEGORIES, \"NUM_CLASSES\": NUM_CLASSES},\n",
    "            }, \"best_graphcnn.pt\")\n",
    "            print(f\"  ✓ Saved checkpoint (val_acc={best_val_acc:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f30952",
   "metadata": {},
   "source": [
    "## Inference / Evaluation on a Single Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e171b3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred idx: [3, 3, 3, 3]\n",
      "GT idx:   [1, 3, 0, 1]\n",
      "Pred synsets: ['03211117', '03211117', '03211117', '03211117']\n",
      "GT synsets:   ['02992529', '03211117', '02808440', '02992529']\n",
      "Pred names:   ['display', 'display', 'display', 'display']\n",
      "GT names:     ['cellphone', 'display', 'bathtub', 'cellphone']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "batch = next(iter(val_loader))\n",
    "meshes = batch[\"meshes\"].to(device)   # now works (Meshes has .to)\n",
    "labels = batch[\"labels\"].to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    logits = model(meshes)\n",
    "    preds = logits.argmax(dim=1)\n",
    "\n",
    "print(\"Pred idx:\", preds.tolist())\n",
    "print(\"GT idx:  \", labels.tolist())\n",
    "print(\"Pred synsets:\", [idx_to_synset[i.item()] for i in preds])\n",
    "print(\"GT synsets:  \", [idx_to_synset[i.item()] for i in labels])\n",
    "print(\"Pred names:  \", [idx_to_name[i.item()]   for i in preds])\n",
    "print(\"GT names:    \", [idx_to_name[i.item()]   for i in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9df6cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on: mps:0\n",
      "Meshes pre-move: <class 'pytorch3d.structures.meshes.Meshes'>\n",
      "Meshes moved to device OK; labels on mps:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model on:\", next(model.parameters()).device)\n",
    "b = next(iter(train_loader))\n",
    "print(\"Meshes pre-move:\", type(b[\"meshes\"]))\n",
    "meshes = b[\"meshes\"].to(device)\n",
    "labels = b[\"labels\"].to(device)\n",
    "print(\"Meshes moved to device OK; labels on\", labels.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7277468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 8\n"
     ]
    }
   ],
   "source": [
    "# --- SMOKE TEST ---\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "SMOKE_N = 64  # try 64 first; bump later\n",
    "train_subset = Subset(train_dataset, list(range(min(SMOKE_N, len(train_dataset)))))\n",
    "val_subset   = Subset(val_dataset,   list(range(min(SMOKE_N//4, len(val_dataset)))))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset, batch_size=2, shuffle=True,\n",
    "    num_workers=0, collate_fn=custom_collate_fn,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset, batch_size=2, shuffle=False,\n",
    "    num_workers=0, collate_fn=custom_collate_fn,\n",
    ")\n",
    "print(len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a7481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class train counts: {'bathtub': None, 'cellphone': None, 'clock': None, 'display': None, 'laptop': None}\n",
      "Per-class val counts:   {'bathtub': None, 'cellphone': None, 'clock': None, 'display': None, 'laptop': None}\n",
      "Train batches: 779 | Val batches: 195 | batch=4\n"
     ]
    }
   ],
   "source": [
    "# Use full datasets? (set True when done debugging)\n",
    "USE_FULL = True\n",
    "\n",
    "# Per-class caps for the stratified subsets (tune as you like)\n",
    "TAKE_PER_CLASS_TRAIN = 300   # try 200–500 depending on speed\n",
    "TAKE_PER_CLASS_VAL   = 100   # make val bigger to stabilize accuracy\n",
    "\n",
    "rng = random.Random(SEED)\n",
    "\n",
    "def stratified_indices(ds, take_per_class):\n",
    "    \"\"\"Return a balanced list of indices for dataset ds based on 'synset_id'.\"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "    for i in range(len(ds)):\n",
    "        s = ds[i][\"synset_id\"]\n",
    "        buckets[s].append(i)\n",
    "    sel = []\n",
    "    for sid in CATEGORIES.keys():\n",
    "        idxs = buckets.get(sid, [])\n",
    "        rng.shuffle(idxs)\n",
    "        sel.extend(idxs[:min(take_per_class, len(idxs))])\n",
    "    rng.shuffle(sel)\n",
    "    return sel, {sid: len([i for i in sel if ds[i][\"synset_id\"] == sid]) for sid in CATEGORIES.keys()}\n",
    "\n",
    "if USE_FULL:\n",
    "    train_dataset_ = train_dataset\n",
    "    val_dataset_   = val_dataset\n",
    "    train_counts   = {sid: None for sid in CATEGORIES.keys()}\n",
    "    val_counts     = {sid: None for sid in CATEGORIES.keys()}\n",
    "else:\n",
    "    train_sel, train_counts = stratified_indices(train_dataset, TAKE_PER_CLASS_TRAIN)\n",
    "    val_sel,   val_counts   = stratified_indices(val_dataset,   TAKE_PER_CLASS_VAL)\n",
    "    train_dataset_ = Subset(train_dataset, train_sel)\n",
    "    val_dataset_   = Subset(val_dataset,   val_sel)\n",
    "\n",
    "print(\"Per-class train counts:\", {CATEGORIES[sid]: train_counts[sid] for sid in CATEGORIES})\n",
    "print(\"Per-class val counts:  \", {CATEGORIES[sid]: val_counts[sid]   for sid in CATEGORIES})\n",
    "\n",
    "# Loader params (MPS/CPU-friendly)\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0\n",
    "PIN = (device.type == \"cuda\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, collate_fn=custom_collate_fn, pin_memory=PIN,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=custom_collate_fn, pin_memory=PIN,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)} | batch={BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c633bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] batch   25/779 | avg 0.39s/batch | ETA 4.9m\n",
      "[train] batch   50/779 | avg 0.40s/batch | ETA 4.8m\n",
      "[train] batch   75/779 | avg 0.42s/batch | ETA 5.0m\n",
      "[train] batch  100/779 | avg 0.40s/batch | ETA 4.5m\n",
      "[train] batch  125/779 | avg 0.40s/batch | ETA 4.4m\n",
      "[train] batch  150/779 | avg 0.40s/batch | ETA 4.2m\n",
      "[train] batch  175/779 | avg 0.40s/batch | ETA 4.1m\n",
      "[train] batch  200/779 | avg 0.40s/batch | ETA 3.9m\n",
      "[train] batch  225/779 | avg 0.42s/batch | ETA 3.9m\n",
      "[train] batch  250/779 | avg 0.43s/batch | ETA 3.8m\n",
      "[train] batch  275/779 | avg 0.43s/batch | ETA 3.6m\n",
      "[train] batch  300/779 | avg 0.45s/batch | ETA 3.6m\n",
      "[train] batch  325/779 | avg 0.45s/batch | ETA 3.4m\n",
      "[train] batch  350/779 | avg 0.46s/batch | ETA 3.3m\n",
      "[train] batch  375/779 | avg 0.47s/batch | ETA 3.1m\n",
      "[train] batch  400/779 | avg 0.48s/batch | ETA 3.0m\n",
      "[train] batch  425/779 | avg 0.49s/batch | ETA 2.9m\n",
      "[train] batch  450/779 | avg 0.50s/batch | ETA 2.7m\n",
      "[train] batch  475/779 | avg 0.51s/batch | ETA 2.6m\n",
      "[train] batch  500/779 | avg 0.52s/batch | ETA 2.4m\n",
      "[train] batch  525/779 | avg 0.53s/batch | ETA 2.3m\n",
      "[train] batch  550/779 | avg 0.55s/batch | ETA 2.1m\n",
      "[train] batch  575/779 | avg 0.56s/batch | ETA 1.9m\n",
      "[train] batch  600/779 | avg 0.58s/batch | ETA 1.7m\n",
      "[train] batch  625/779 | avg 0.59s/batch | ETA 1.5m\n",
      "[train] batch  650/779 | avg 0.60s/batch | ETA 1.3m\n",
      "[train] batch  675/779 | avg 0.61s/batch | ETA 1.1m\n",
      "[train] batch  700/779 | avg 0.62s/batch | ETA 0.8m\n",
      "[train] batch  725/779 | avg 0.63s/batch | ETA 0.6m\n",
      "[train] batch  750/779 | avg 0.65s/batch | ETA 0.3m\n",
      "[train] batch  775/779 | avg 0.66s/batch | ETA 0.0m\n",
      "[train] batch  779/779 | avg 0.66s/batch | ETA 0.0m\n",
      "[val  ] batch   25/195 | avg 1.11s/batch | ETA 3.1m\n",
      "[val  ] batch   50/195 | avg 1.09s/batch | ETA 2.6m\n",
      "[val  ] batch   75/195 | avg 1.10s/batch | ETA 2.2m\n",
      "[val  ] batch  100/195 | avg 1.11s/batch | ETA 1.8m\n",
      "[val  ] batch  125/195 | avg 1.15s/batch | ETA 1.3m\n",
      "[val  ] batch  150/195 | avg 1.17s/batch | ETA 0.9m\n",
      "[val  ] batch  175/195 | avg 1.18s/batch | ETA 0.4m\n",
      "[val  ] batch  195/195 | avg 1.21s/batch | ETA 0.0m\n",
      "Epoch 01/4 | train_loss=1.2419 acc=0.502 | val_loss=1.1414 acc=0.589 | 753.3s\n",
      "  ✓ Saved checkpoint (val_acc=0.589)\n",
      "[train] batch   25/779 | avg 1.34s/batch | ETA 16.9m\n",
      "[train] batch   50/779 | avg 1.39s/batch | ETA 16.9m\n",
      "[train] batch   75/779 | avg 1.43s/batch | ETA 16.8m\n",
      "[train] batch  100/779 | avg 1.50s/batch | ETA 17.0m\n",
      "[train] batch  125/779 | avg 1.53s/batch | ETA 16.7m\n",
      "[train] batch  150/779 | avg 1.56s/batch | ETA 16.4m\n",
      "[train] batch  175/779 | avg 1.59s/batch | ETA 16.0m\n",
      "[train] batch  200/779 | avg 1.62s/batch | ETA 15.7m\n",
      "[train] batch  225/779 | avg 1.67s/batch | ETA 15.4m\n",
      "[train] batch  250/779 | avg 1.71s/batch | ETA 15.0m\n",
      "[train] batch  275/779 | avg 1.78s/batch | ETA 14.9m\n",
      "[train] batch  300/779 | avg 1.83s/batch | ETA 14.6m\n",
      "[train] batch  325/779 | avg 1.87s/batch | ETA 14.2m\n",
      "[train] batch  350/779 | avg 1.91s/batch | ETA 13.7m\n",
      "[train] batch  375/779 | avg 1.93s/batch | ETA 13.0m\n",
      "[train] batch  400/779 | avg 1.96s/batch | ETA 12.4m\n",
      "[train] batch  425/779 | avg 1.99s/batch | ETA 11.7m\n",
      "[train] batch  450/779 | avg 2.02s/batch | ETA 11.1m\n",
      "[train] batch  475/779 | avg 2.05s/batch | ETA 10.4m\n",
      "[train] batch  500/779 | avg 2.08s/batch | ETA 9.7m\n",
      "[train] batch  525/779 | avg 2.11s/batch | ETA 8.9m\n",
      "[train] batch  550/779 | avg 2.13s/batch | ETA 8.1m\n",
      "[train] batch  575/779 | avg 2.15s/batch | ETA 7.3m\n",
      "[train] batch  600/779 | avg 2.18s/batch | ETA 6.5m\n",
      "[train] batch  625/779 | avg 2.21s/batch | ETA 5.7m\n",
      "[train] batch  650/779 | avg 2.25s/batch | ETA 4.8m\n",
      "[train] batch  675/779 | avg 2.28s/batch | ETA 4.0m\n",
      "[train] batch  700/779 | avg 2.31s/batch | ETA 3.0m\n",
      "[train] batch  725/779 | avg 2.33s/batch | ETA 2.1m\n",
      "[train] batch  750/779 | avg 2.37s/batch | ETA 1.1m\n",
      "[train] batch  775/779 | avg 2.40s/batch | ETA 0.2m\n",
      "[train] batch  779/779 | avg 2.41s/batch | ETA 0.0m\n",
      "[val  ] batch   25/195 | avg 2.74s/batch | ETA 7.8m\n",
      "[val  ] batch   50/195 | avg 2.97s/batch | ETA 7.2m\n",
      "[val  ] batch   75/195 | avg 3.04s/batch | ETA 6.1m\n",
      "[val  ] batch  100/195 | avg 3.11s/batch | ETA 4.9m\n",
      "[val  ] batch  125/195 | avg 3.21s/batch | ETA 3.7m\n",
      "[val  ] batch  150/195 | avg 3.33s/batch | ETA 2.5m\n",
      "[val  ] batch  175/195 | avg 3.36s/batch | ETA 1.1m\n",
      "[val  ] batch  195/195 | avg 3.46s/batch | ETA 0.0m\n",
      "Epoch 02/4 | train_loss=1.0362 acc=0.623 | val_loss=1.1327 acc=0.626 | 2552.5s\n",
      "  ✓ Saved checkpoint (val_acc=0.626)\n",
      "[train] batch   25/779 | avg 3.90s/batch | ETA 49.0m\n",
      "[train] batch   50/779 | avg 4.00s/batch | ETA 48.5m\n",
      "[train] batch   75/779 | avg 4.03s/batch | ETA 47.3m\n",
      "[train] batch  100/779 | avg 4.22s/batch | ETA 47.7m\n",
      "[train] batch  125/779 | avg 4.40s/batch | ETA 48.0m\n",
      "[train] batch  150/779 | avg 4.44s/batch | ETA 46.5m\n",
      "[train] batch  175/779 | avg 4.48s/batch | ETA 45.1m\n",
      "[train] batch  200/779 | avg 4.56s/batch | ETA 44.0m\n",
      "[train] batch  225/779 | avg 4.63s/batch | ETA 42.7m\n",
      "[train] batch  250/779 | avg 4.65s/batch | ETA 41.0m\n",
      "[train] batch  275/779 | avg 4.59s/batch | ETA 38.5m\n",
      "[train] batch  300/779 | avg 4.63s/batch | ETA 36.9m\n",
      "[train] batch  325/779 | avg 4.65s/batch | ETA 35.2m\n",
      "[train] batch  350/779 | avg 4.67s/batch | ETA 33.4m\n",
      "[train] batch  375/779 | avg 4.69s/batch | ETA 31.6m\n",
      "[train] batch  400/779 | avg 4.72s/batch | ETA 29.8m\n",
      "[train] batch  425/779 | avg 4.77s/batch | ETA 28.2m\n",
      "[train] batch  450/779 | avg 4.78s/batch | ETA 26.2m\n",
      "[train] batch  475/779 | avg 4.84s/batch | ETA 24.5m\n",
      "[train] batch  500/779 | avg 4.89s/batch | ETA 22.7m\n",
      "[train] batch  525/779 | avg 4.92s/batch | ETA 20.8m\n",
      "[train] batch  550/779 | avg 4.99s/batch | ETA 19.1m\n",
      "[train] batch  575/779 | avg 5.17s/batch | ETA 17.6m\n",
      "[train] batch  600/779 | avg 5.24s/batch | ETA 15.6m\n",
      "[train] batch  625/779 | avg 5.52s/batch | ETA 14.2m\n",
      "[train] batch  650/779 | avg 5.54s/batch | ETA 11.9m\n",
      "[train] batch  675/779 | avg 5.62s/batch | ETA 9.7m\n",
      "[train] batch  700/779 | avg 5.71s/batch | ETA 7.5m\n",
      "[train] batch  725/779 | avg 5.81s/batch | ETA 5.2m\n",
      "[train] batch  750/779 | avg 6.20s/batch | ETA 3.0m\n",
      "[train] batch  775/779 | avg 6.26s/batch | ETA 0.4m\n",
      "[train] batch  779/779 | avg 6.26s/batch | ETA 0.0m\n",
      "[val  ] batch   25/195 | avg 8.79s/batch | ETA 24.9m\n",
      "[val  ] batch   50/195 | avg 7.90s/batch | ETA 19.1m\n",
      "[val  ] batch   75/195 | avg 7.86s/batch | ETA 15.7m\n",
      "[val  ] batch  100/195 | avg 7.75s/batch | ETA 12.3m\n",
      "[val  ] batch  125/195 | avg 7.62s/batch | ETA 8.9m\n",
      "[val  ] batch  150/195 | avg 7.59s/batch | ETA 5.7m\n",
      "[val  ] batch  175/195 | avg 7.45s/batch | ETA 2.5m\n",
      "[val  ] batch  195/195 | avg 7.45s/batch | ETA 0.0m\n",
      "Epoch 03/4 | train_loss=1.0010 acc=0.665 | val_loss=1.0457 acc=0.654 | 6333.1s\n",
      "  ✓ Saved checkpoint (val_acc=0.654)\n",
      "[train] batch   25/779 | avg 7.91s/batch | ETA 99.4m\n",
      "[train] batch   50/779 | avg 7.92s/batch | ETA 96.2m\n",
      "[train] batch   75/779 | avg 11.17s/batch | ETA 131.1m\n",
      "[train] batch  100/779 | avg 10.93s/batch | ETA 123.6m\n",
      "[train] batch  125/779 | avg 11.00s/batch | ETA 119.9m\n",
      "[train] batch  150/779 | avg 11.78s/batch | ETA 123.5m\n",
      "[train] batch  175/779 | avg 11.86s/batch | ETA 119.4m\n",
      "[train] batch  200/779 | avg 12.38s/batch | ETA 119.5m\n",
      "[train] batch  225/779 | avg 12.50s/batch | ETA 115.4m\n",
      "[train] batch  250/779 | avg 12.86s/batch | ETA 113.4m\n",
      "[train] batch  275/779 | avg 12.86s/batch | ETA 108.1m\n",
      "[train] batch  300/779 | avg 12.85s/batch | ETA 102.6m\n",
      "[train] batch  325/779 | avg 12.39s/batch | ETA 93.7m\n",
      "[train] batch  350/779 | avg 12.86s/batch | ETA 91.9m\n",
      "[train] batch  375/779 | avg 13.89s/batch | ETA 93.5m\n",
      "[train] batch  400/779 | avg 14.26s/batch | ETA 90.1m\n",
      "[train] batch  425/779 | avg 15.30s/batch | ETA 90.3m\n",
      "[train] batch  450/779 | avg 14.78s/batch | ETA 81.0m\n",
      "[train] batch  475/779 | avg 14.33s/batch | ETA 72.6m\n",
      "[train] batch  500/779 | avg 13.94s/batch | ETA 64.8m\n",
      "[train] batch  525/779 | avg 14.00s/batch | ETA 59.3m\n",
      "[train] batch  550/779 | avg 15.23s/batch | ETA 58.1m\n",
      "[train] batch  575/779 | avg 15.65s/batch | ETA 53.2m\n",
      "[train] batch  600/779 | avg 16.05s/batch | ETA 47.9m\n",
      "[train] batch  625/779 | avg 16.30s/batch | ETA 41.8m\n",
      "[train] batch  650/779 | avg 16.77s/batch | ETA 36.1m\n",
      "[train] batch  675/779 | avg 17.69s/batch | ETA 30.7m\n",
      "[train] batch  700/779 | avg 18.31s/batch | ETA 24.1m\n",
      "[train] batch  725/779 | avg 18.94s/batch | ETA 17.0m\n",
      "[train] batch  750/779 | avg 20.72s/batch | ETA 10.0m\n",
      "[train] batch  775/779 | avg 23.51s/batch | ETA 1.6m\n",
      "[train] batch  779/779 | avg 24.66s/batch | ETA 0.0m\n",
      "[val  ] batch   25/195 | avg 123.09s/batch | ETA 348.8m\n",
      "[val  ] batch   50/195 | avg 116.60s/batch | ETA 281.8m\n",
      "[val  ] batch   75/195 | avg 113.23s/batch | ETA 226.5m\n",
      "[val  ] batch  100/195 | avg 111.40s/batch | ETA 176.4m\n",
      "[val  ] batch  125/195 | avg 103.96s/batch | ETA 121.3m\n",
      "[val  ] batch  150/195 | avg 99.21s/batch | ETA 74.4m\n",
      "[val  ] batch  175/195 | avg 98.55s/batch | ETA 32.8m\n",
      "[val  ] batch  195/195 | avg 102.56s/batch | ETA 0.0m\n",
      "Epoch 04/4 | train_loss=0.9462 acc=0.696 | val_loss=0.9893 acc=0.717 | 39206.3s\n",
      "  ✓ Saved checkpoint (val_acc=0.717)\n"
     ]
    }
   ],
   "source": [
    "train_loop(epochs=4)   # try 10–30 on MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5b8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e58c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from checkpoint.\n",
      "Validation accuracy: 0.733\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"best_graphcnn.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "print(\"Loaded model weights from checkpoint.\")\n",
    "\n",
    "# Full validation accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.inference_mode():\n",
    "    for batch in val_loader:\n",
    "        meshes = batch[\"meshes\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        logits = model(meshes)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "val_acc = correct / max(1, total)\n",
    "print(f\"Validation accuracy: {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6f245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix rows=GT, cols=Pred:\n",
      " tensor([[145,   0,   0,   9,   7],\n",
      "        [  0, 137,   4,  33,   0],\n",
      "        [  8,  33,  45,  45,   1],\n",
      "        [  6,   9,   3, 198,   7],\n",
      "        [ 31,   0,   0,   8,  49]])\n",
      "Per-class accuracy:\n",
      "  0: bathtub  acc=0.901\n",
      "  1: cellphone  acc=0.787\n",
      "  2: clock  acc=0.341\n",
      "  3: display  acc=0.888\n",
      "  4: laptop  acc=0.557\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "num_classes = NUM_CLASSES\n",
    "cm = torch.zeros((num_classes, num_classes), dtype=torch.long)\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for batch in val_loader:\n",
    "        meshes = batch[\"meshes\"].to(device)\n",
    "        labels = batch[\"labels\"]\n",
    "        preds = model(meshes).argmax(1).cpu()\n",
    "        for t, p in zip(labels, preds):\n",
    "            cm[t, p] += 1\n",
    "\n",
    "print(\"Confusion matrix rows=GT, cols=Pred:\\n\", cm)\n",
    "print(\"Per-class accuracy:\")\n",
    "for i in range(num_classes):\n",
    "    denom = cm[i].sum().item()\n",
    "    acc_i = (cm[i, i].item() / denom) if denom > 0 else float(\"nan\")\n",
    "    print(f\"  {i}: {idx_to_name[i]}  acc={acc_i:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdef9d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
